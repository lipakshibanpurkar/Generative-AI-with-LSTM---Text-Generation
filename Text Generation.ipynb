{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNh7qwSiXuwus16qPxjCoR6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Generative AI with LSTM - Text Generation\n"],"metadata":{"id":"rk-BfVlyZpbZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UCW5poy7TogG","outputId":"c56433c7-174c-459c-c05a-26cf981461aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Loading dataset from Project Gutenberg...\n","============================================================\n","âœ“ Dataset downloaded successfully\n","Using optimized dataset: 500,000 characters\n","\n","Dataset loaded. Text length: 485,131 characters\n","Sample preview: the complete works of william shakespeare by william shakespeare contents the sonnets alls well that ends well the tragedy of antony and cleopatra as ...\n","\n","Tokenizing text...\n","âœ“ Tokenization complete. Total unique words: 8325\n","Time: 0.05 seconds\n","\n","Creating training sequences...\n","Created 89,829 training sequences (optimized)\n","X shape: (89829, 20), y shape: (89829, 8325)\n","Time: 0.63 seconds\n","\n","Building optimized model...\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚       \u001b[38;5;34m532,800\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m98,816\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m131,584\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8325\u001b[0m)           â”‚       \u001b[38;5;34m541,125\u001b[0m â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">532,800</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8325</span>)           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">541,125</span> â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,312,581\u001b[0m (5.01 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,581</span> (5.01 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,312,581\u001b[0m (5.01 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,581</span> (5.01 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","TRAINING MODEL (Optimized for Speed)\n","============================================================\n","Estimated time: 20-40 minutes (was 2-5 hours)\n","============================================================\n","\n","Training started at: 22:18:10\n","\n","==================================================\n","Epoch 1/10\n","==================================================\n","Epoch 1 completed\n","Loss: 6.8673, Accuracy: 0.028313\n","Val Loss: 6.6107, Val Accuracy: 0.028053\n","==================================================\n","\n","==================================================\n","Epoch 2/10\n","==================================================\n","Epoch 2 completed\n","Loss: 6.5580, Accuracy: 0.035067\n","Val Loss: 6.5510, Val Accuracy: 0.041300\n","==================================================\n","\n","==================================================\n","Epoch 3/10\n","==================================================\n","Epoch 3 completed\n","Loss: 6.4193, Accuracy: 0.040460\n","Val Loss: 6.5429, Val Accuracy: 0.038517\n","==================================================\n","\n","==================================================\n","Epoch 4/10\n","==================================================\n"]}],"source":["\n","import requests\n","import re\n","import numpy as np\n","import tensorflow as tf\n","import os\n","import time\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n","from tensorflow.keras.layers import Input\n","import pickle\n","\n","# -------------------------1. Load Dataset -------------------------\n","print(\"=\"*60)\n","print(\"Loading dataset from Project Gutenberg...\")\n","print(\"=\"*60)\n","\n","url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n","try:\n","    response = requests.get(url, timeout=30)\n","    response.raise_for_status()\n","    text = response.text.lower()\n","    print(\"âœ“ Dataset downloaded successfully\")\n","except Exception as e:\n","    print(f\"âœ— Error downloading: {e}\")\n","    exit(1)\n","\n","start = text.find(\"the complete works of william shakespeare\")\n","end = text.find(\"end of the project gutenberg ebook\")\n","\n","if start == -1 or end == -1:\n","    print(\"Warning: Could not find expected markers, using first 1M characters\")\n","    start = 0\n","    end = 1000000\n","\n","text = text[start:end]\n","\n","\n","text = text[:500000]\n","print(f\"Using optimized dataset: {len(text):,} characters\")\n","\n","\n","text = re.sub(r\"[^a-z\\s.,!?;:]\", \"\", text)\n","text = re.sub(r\"\\s+\", \" \", text).strip()\n","\n","print(f\"\\nDataset loaded. Text length: {len(text):,} characters\")\n","print(f\"Sample preview: {text[:150]}...\\n\")\n","\n","# -------------------------2. Tokenization -------------------------\n","print(\"Tokenizing text...\")\n","start_time = time.time()\n","\n","tokenizer = Tokenizer(num_words=10000)\n","tokenizer.fit_on_texts([text])\n","\n","total_words = min(len(tokenizer.word_index) + 1, 10001)\n","print(f\"âœ“ Tokenization complete. Total unique words: {total_words}\")\n","print(f\"Time: {time.time() - start_time:.2f} seconds\\n\")\n","\n","# ------------------------- 3. Create Sequences -------------------------\n","print(\"Creating training sequences...\")\n","start_time = time.time()\n","\n","sequences = tokenizer.texts_to_sequences([text])[0]\n","\n","sequence_length = 20\n","input_sequences = []\n","\n","for i in range(sequence_length, len(sequences)):\n","    seq = sequences[i-sequence_length:i+1]\n","    input_sequences.append(seq)\n","\n","MAX_SEQUENCES = 200000\n","if len(input_sequences) > MAX_SEQUENCES:\n","    input_sequences = input_sequences[:MAX_SEQUENCES]\n","\n","print(f\"Created {len(input_sequences):,} training sequences (optimized)\")\n","\n","\n","input_sequences = np.array(input_sequences, dtype=np.int32)\n","X = input_sequences[:, :-1]\n","y = input_sequences[:, -1]\n","\n","\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n","\n","print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n","print(f\"Time: {time.time() - start_time:.2f} seconds\\n\")\n","\n","# -------------------------4. Build Model -------------------------\n","print(\"Building optimized model...\")\n","model = Sequential([\n","    Input(shape=(sequence_length,)),\n","    Embedding(total_words, 64),\n","    LSTM(128, return_sequences=True),\n","    LSTM(128),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dense(total_words, activation=\"softmax\")\n","])\n","\n","model.compile(\n","    loss=\"categorical_crossentropy\",\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","print()\n","\n","# ------------------------- 5. Custom Callback for Progress-------------------------\n","class TrainingProgress(Callback):\n","    def on_epoch_begin(self, epoch, logs=None):\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Epoch {epoch+1}/10\")\n","        print(f\"{'='*50}\")\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(f\"Epoch {epoch+1} completed\")\n","        print(f\"Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.6f}\")\n","        if 'val_loss' in logs:\n","            print(f\"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.6f}\")\n","        print(f\"{'='*50}\")\n","\n","# ------------------------- 6. Train Model -------------------------\n","print(\"=\"*60)\n","print(\"TRAINING MODEL (Optimized for Speed)\")\n","print(\"=\"*60)\n","print(\"Estimated time: 20-40 minutes (was 2-5 hours)\")\n","print(\"=\"*60)\n","\n","checkpoint = ModelCheckpoint(\n","    'best_shakespeare_model.keras',\n","    monitor='val_loss',\n","    save_best_only=True,\n","    save_weights_only=False,\n","    verbose=0\n",")\n","\n","early_stop = EarlyStopping(\n","    monitor=\"val_loss\",\n","    patience=3,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","progress = TrainingProgress()\n","\n","start_time = time.time()\n","print(f\"\\nTraining started at: {time.strftime('%H:%M:%S')}\")\n","\n","history = model.fit(\n","    X,\n","    y,\n","    epochs=10,\n","    batch_size=128,\n","    validation_split=0.1,\n","    callbacks=[early_stop, checkpoint, progress],\n","    verbose=0\n",")\n","\n","training_time = time.time() - start_time\n","print(f\"\\n{'='*60}\")\n","print(f\"TRAINING COMPLETED SUCCESSFULLY!\")\n","print(f\"Total training time: {training_time/60:.1f} minutes\")\n","print(f\"{'='*60}\")\n","\n","# ------------------------- 7. Text Generation Function -------------------------\n","def generate_text(seed_text, next_words=20, temperature=0.8, diversity=1.0):\n","    \"\"\"\n","    Generate text starting from seed_text.\n","\n","    Parameters:\n","    - seed_text: Starting text\n","    - next_words: Number of words to generate\n","    - temperature: Controls randomness (0.1-2.0)\n","    - diversity: Additional randomness factor\n","    \"\"\"\n","    generated_text = seed_text\n","\n","    for word_num in range(next_words):\n","\n","        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n","\n","        if len(token_list) > sequence_length:\n","            token_list = token_list[-sequence_length:]\n","\n","\n","        token_list = pad_sequences(\n","            [token_list], maxlen=sequence_length, padding=\"pre\"\n","        )\n","\n","        predictions = model.predict(token_list, verbose=0)[0]\n","\n","        predictions = np.log(predictions + 1e-7) / temperature\n","        predictions = predictions * diversity\n","        exp_preds = np.exp(predictions)\n","        predictions = exp_preds / np.sum(exp_preds)\n","\n","        predicted_index = np.random.choice(range(total_words), p=predictions)\n","\n","        output_word = tokenizer.index_word.get(predicted_index, \"\")\n","\n","        if output_word:\n","            generated_text += \" \" + output_word\n","\n","            if word_num % 8 == 0 and word_num > 0:\n","                generated_text += \"\\n\"\n","\n","    return generated_text\n","\n","# -------------------------8. Generate Sample Outputs-------------------------\n","print(\"\\n\" + \"=\"*60)\n","print(\"GENERATING SHAKESPEARE TEXT\")\n","print(\"=\"*60)\n","\n","seed_texts = [\n","    \"to be or not to be\",\n","    \"romeo romeo wherefore art thou\",\n","    \"all the world s a stage\",\n","    \"shall i compare thee to a\",\n","    \"now is the winter of our\"\n","]\n","\n","print(\"\\nGenerating text samples (this may take a moment)...\\n\")\n","\n","for i, seed in enumerate(seed_texts, 1):\n","    print(f\"{i}. Seed: '{seed}'\")\n","    print(\"-\" * 50)\n","\n","    generated = generate_text(seed, next_words=25, temperature=0.7)\n","\n","    lines = generated.split('\\n')\n","    for line in lines:\n","\n","        words = line.strip().split()\n","        if words:\n","            words[0] = words[0].capitalize()\n","            print(' '.join(words))\n","\n","    print()\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"SAVING MODEL AND TOKENIZER\")\n","print(\"=\"*60)\n","\n","\n","model.save(\"shakespeare_lstm_optimized.keras\")\n","print(\"âœ“ Model saved: 'shakespeare_lstm_optimized.keras'\")\n","\n","with open('shakespeare_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","print(\"âœ“ Tokenizer saved: 'shakespeare_tokenizer.pickle'\")\n","\n","training_history = {\n","    'loss': history.history['loss'],\n","    'accuracy': history.history['accuracy'],\n","    'val_loss': history.history['val_loss'],\n","    'val_accuracy': history.history['val_accuracy']\n","}\n","\n","with open('training_history.pickle', 'wb') as handle:\n","    pickle.dump(training_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","print(\"âœ“ Training history saved: 'training_history.pickle'\")\n","\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"TRAINING SUMMARY\")\n","print(\"=\"*60)\n","\n","final_epoch = len(history.history['loss'])\n","final_loss = history.history['loss'][-1]\n","final_acc = history.history['accuracy'][-1]\n","final_val_loss = history.history['val_loss'][-1]\n","final_val_acc = history.history['val_accuracy'][-1]\n","\n","print(f\"Model Architecture: LSTM with {model.count_params():,} parameters\")\n","print(f\"Vocabulary Size: {total_words:,} words\")\n","print(f\"Training Sequences: {len(X):,}\")\n","print(f\"Sequence Length: {sequence_length} words\")\n","print(f\"Epochs Trained: {final_epoch}\")\n","print(f\"Final Training Loss: {final_loss:.4f}\")\n","print(f\"Final Training Accuracy: {final_acc:.6f}\")\n","print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n","print(f\"Final Validation Accuracy: {final_val_acc:.6f}\")\n","print(f\"Total Training Time: {training_time/60:.1f} minutes\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"HOW TO USE YOUR TRAINED MODEL\")\n","print(\"=\"*60)\n","print(\"1. To generate more text, call:\")\n","print(\"   generate_text('your seed text here', next_words=30)\")\n","print(\"\\n2. To load and use the saved model later:\")\n","print(\"   from tensorflow.keras.models import load_model\")\n","print(\"   model = load_model('shakespeare_lstm_optimized.keras')\")\n","print(\"\\n3. To experiment with creativity:\")\n","print(\"   generate_text('to be', temperature=0.5)  # More conservative\")\n","print(\"   generate_text('to be', temperature=1.5)  # More creative\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"SCRIPT COMPLETED SUCCESSFULLY! ğŸ­\")\n","print(\"=\"*60)"]},{"cell_type":"code","source":[],"metadata":{"id":"9sXdmXktUiOx"},"execution_count":null,"outputs":[]}]}